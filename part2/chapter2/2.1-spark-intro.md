# 2.1 Spark Intro

## Spark简介

### 什么是Spark

Spark是一种快速、通用、可扩展的大数据分析引擎（框架），2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache的顶级项目。

![](../../.gitbook/assets/0%20%281%29.png)

### Spark的特点

Spark的API是高度封装的，我们只需要写函数，把相应的处理逻辑传进去就可以了。

### Spark与MapReduce的对比

| **框架** | **优点** | **缺点** |
| :--- | :--- | :--- |
| MapReduce | 历史悠久、稳定 | 编程API不灵活、速度慢、只能做离线计算 |
| Spark | 通用、编程API简洁、快 | 跟MapReduce比暂无缺点 |

## Spark架构体系

![Spark&#x5404;&#x4E2A;&#x89D2;&#x8272;&#x7684;&#x4ECB;&#x7ECD;](../../.gitbook/assets/1%20%282%29.png)

### Spark中重要角色

* **Master** ：是一个Java进程，接收Worker的注册信息和心跳、移除异常超时的Worker、接收客户端提交的任务、负责资源调度、命令Worker启动Executor。
* **Worker** ：是一个Java进程，负责管理当前节点的资源关联，向Master注册并定期发送心跳。Worker自己不执行进程，负责启动Executor（Executor向SparkSubmit反向注册，真正进行计算执行任务），并监控Executor的状态。以后任务是运行在`CoarseGrainedExecutorBackend`的进程里。
* **SparkSubmit** ：是一个Java进程，负责向Master提交任务。
* **Driver ：**是很多类的统称，可以认为SparkContext就是Driver，client模式Driver运行在SparkSubmit进程中，cluster模式单独运行在一个进程中，负责将用户编写的代码转成Tasks，然后调度到Executor中执行，并监控Task的状态和执行进度。
* **Executor** ：是一个Java进程，负责执行Driver端生成的Task，将Task放入线程中运行。

### 分布式计算程序

![](../../.gitbook/assets/fen-bu-shi-ji-suan-cheng-xu-%20%281%29.png)

### Spark和Yarn角色对比

| **Spark Standalone的Client模式** | **YARN** |
| :--- | :--- |
| Master | ResourceManager |
| Worker | NodeManager |
| Executor | YarnChild |
| SparkSubmit（Driver） | ApplicationMaster |

## Spark环境搭建

### 架构说明

**（standalone模式）**

standalone模式是Spark自带的分布式集群模式，不依赖其他的资源调度框架。

![spark&#x6D41;&#x7A0B;&#x56FE;&#x57FA;&#x7840;&#x7248;&#x672C; \(1\)](../../.gitbook/assets/2%20%281%29.png)

### 搭建步骤

* 下载Spark安装包，下载地址：[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)

![](../../.gitbook/assets/3%20%281%29.png)

* 上传spark安装包到Linux服务器上
* 解压spark安装包：

`tar -zxvf spark-2.3.3-bin-hadoop2.7.tgz -C /bigdata/`

* 将conf目录下的`spark-env.sh.template`重命名为`spark-env.sh`，并修改：

`export JAVA_HOME=/opt/jdk1.8.0_251`

`export SPARK_MASTER_HOST=miao1`

* 将conf目录下的slaves.template重命名为slaves并修改，指定Worker的所在节点：

`miao2`

`miao3`

* 将配置好的Spark拷贝到其他节点：

`for i in {2..3}; do scp -r spark-2.3.3 node-$i.51doit.cn:$PWD; done`

### 启动Spark集群

* 在Spark的安装目录执行启动脚本
* 执行jps命令查看Java进程

在miao1上可用看见Master进程，在其他的节点上可用看见到Worker

* 访问Master的web管理界面，端口8080： [http://miao1:8080/](http://miao1:8080/)

![](../../.gitbook/assets/image%20%285%29.png)

## 启动Spark Shell编程

### 什么是Spark Shell

Spark Shell是Spark中的交互式命令行客户端，可以在Spark Shell中使用Scala编写Spark程序，启动后默认已经创建了SparkContext，别名为sc。

### 启动Spark Shell

`[root@miao1 spark-2.4.5-bin-hadoop2.7]# bin/spark-shell --master spark:// miao1:7077 --executor-memory 512m --total-executor-cores 3`

参数说明：

`--master` 指定master的地址和端口，协议为spark://，端口7077是RPC的通信端口。如果不指定会在local模式运行，没有把任务提交到集群，因为它找不到master是谁。

`--executor-memory` 指定每一个executor的使用的内存大小，每个机器默认用1G。

`--total-executor-cores` 指定整个application总共使用了cores，如果不指定会默认把集群所有的核都用了。

![](../../.gitbook/assets/image%20%286%29.png)

### 在shell中编写第一个spark程序

`scala> sc.textFile("hdfs://miao1:9000/word.txt").flatMap(.split(" ")).map((,1)).reduceByKey(+).sortBy(_._2,false).collect`

`res5: Array[(String, Int)] = Array((hello,4), (tom,2), (jerry,2))`

或者是将结果写到HDFS：

`sc.textFile("hdfs://miao1:9000/word.txt").flatMap(.split(" ")).map((,1)).reduceByKey(+).sortBy(_._2,false).saveAsTextFile("hdfs://miao1:9000/out11")`

![](../../.gitbook/assets/image%20%288%29.png)

