# 2.1 Spark Intro

## Spark简介

### 什么是Spark

Spark是一种快速、通用、可扩展的大数据分析引擎（框架），2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache的顶级项目。

![](../../.gitbook/assets/0%20%281%29.png)

### Spark的特点

Spark的API是高度封装的，我们只需要写函数，把相应的处理逻辑传进去就可以了。

### Spark与MapReduce的对比

| **框架** | **优点** | **缺点** |
| :--- | :--- | :--- |
| MapReduce | 历史悠久、稳定 | 编程API不灵活、速度慢、只能做离线计算 |
| Spark | 通用、编程API简洁、快 | 跟MapReduce比暂无缺点 |

## Spark架构体系

![Spark&#x5404;&#x4E2A;&#x89D2;&#x8272;&#x7684;&#x4ECB;&#x7ECD;](../../.gitbook/assets/1%20%282%29.png)

### Spark中重要角色

* **Master** ：是一个Java进程，接收Worker的注册信息和心跳、移除异常超时的Worker、接收客户端提交的任务、负责资源调度、命令Worker启动Executor。
* **Worker** ：是一个Java进程，负责管理当前节点的资源关联，向Master注册并定期发送心跳，负责启动Executor、并监控Executor的状态。
* **SparkSubmit** ：是一个Java进程，负责向Master提交任务。
* **Driver ：**是很多类的统称，可以认为SparkContext就是Driver，client模式Driver运行在SparkSubmit进程中，cluster模式单独运行在一个进程中，负责将用户编写的代码转成Tasks，然后调度到Executor中执行，并监控Task的状态和执行进度。
* **Executor** ：是一个Java进程，负责执行Driver端生成的Task，将Task放入线程中运行。

### Spark和Yarn角色对比

| **Spark Standalone的Client模式** | **YARN** |
| :--- | :--- |
| Master | ResourceManager |
| Worker | NodeManager |
| Executor | YarnChild |
| SparkSubmit（Driver） | ApplicationMaster |

## Spark环境搭建

### 架构说明（standalone模式）

standalone模式是Spark自带的分布式集群模式，不依赖其他的资源调度框架

![spark&#x6D41;&#x7A0B;&#x56FE;&#x57FA;&#x7840;&#x7248;&#x672C; \(1\)](../../.gitbook/assets/2%20%281%29.png)

### 搭建步骤

* 下载Spark安装包，下载地址：[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)

![](../../.gitbook/assets/3%20%281%29.png)

* 上传spark安装包到Linux服务器上
* 解压spark安装包

`tar -zxvf spark-2.3.3-bin-hadoop2.7.tgz -C /bigdata/`

* 将conf目录下的spark-env.sh.template重命名为spark-env.sh，并修改

`export JAVA_HOME=/usr/local/jdk1.8.0_192`

`export SPARK_MASTER_HOST=node-1.51doit.cn`

* 将conf目录下的slaves.template重命名为slaves并修改，指定Worker的所在节点

node-2.51doit.cn

node-3.51doit.cn

* 将配置好的Spark拷贝到其他节点

`for i in {2..3}; do scp -r spark-2.3.3 node-$i.51doit.cn:$PWD; done`

### 启动Spark集群

* 在Spark的安装目录执行启动脚本
* 执行jps命令查看Java进程

在ndoe-1上可用看见Master进程，在其他的节点上可用看见到Worker

* 访问Master的web管理界面，端口8080

![](../../.gitbook/assets/4.png)

## 启动Spark Shell编程

### 什么是Spark Shell

spark shell是spark中的交互式命令行客户端，可以在spark shell中使用scala编写spark程序，启动后默认已经创建了SparkContext，别名为sc

### 启动Spark Shell

/bigdata/spark-2.3.3-bin-hadoop2.7/bin/spark-shell --master spark://node-1.51doit.cn:7077 --executor-memory 800m --total-executor-cores 3

参数说明：

**--master** 指定masterd地址和端口，协议为spark://，端口是RPC的通信端口

**--executor-memory** 指定每一个executor的使用的内存大小

**--total-executor-cores**指定整个application总共使用了cores

![](../../.gitbook/assets/5.png)

* 在shell中编写第一个spark程序

sc.textFile\("hdfs://node-1.51doit.cn:9000/words.txt"\).flatMap\(\_.split\(" "\)\).map\(\(\_, 1\)\).reduceByKey\(\_+\_\).sortBy\(\_.\_2,false\).saveAsTextFile\("hdfs://node-1.51doit.cn:9000/out11"\)

